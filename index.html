---
layout: default
tags: about
---
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
<script type="text/javascript">
function readMore() {
    $('#readMore').hide();
    $('#more').show();
}
function readLess() {
    $('#readMore').show();
    $('#more').hide();
}
</script> 

<img src="images/Me.jpg" alt="Sri Siddarth C" width="240" style="float: right; padding: 0px; border-radius: 100%;" />
<div class="bio">
  <!-- <hr/> style="text-align:justify">-->
  <p>I am currently a Research Assistant in the <a href="https://lab1055.github.io/" class="uline">Machine Learning and Vision Group</a> at the Indian Institute of Technology Hyderabad (IITH), under the guidance of <a href="https://people.iith.ac.in/vineethnb/" class="uline"> Dr. Vineeth N Balasubramanian</a>. 
  My research revolves around <b><i>Explainable AI</i></b>, <b><i>Continual Learning</i></b>, and <b><i>Multi-Modal Learning</i></b>.

  <br><br>
  Previously, I collaborated with <a href="https://aero.iisc.ac.in/people/suresh-sundaram/" class="uline">Dr. Suresh Sundaram</a> at the Indian Institute of Science, extending our interactions from Nanyang Technological University, Singapore. I worked in the <a href="https://in.linkedin.com/company/artificial-intelligence-and-robotics-laboratory" class="uline">Artificial Intelligence and Robotics Laboratory</a> (AIRL) on Motion Planning for ground robotics and 3D reconstruction. I also helped develop an AI-enabled autonomous drone for surveillance using light-weight object detection models for real-time inference. 
  <br><br>
  I received my Bachelor's in Computer Science and Engineering from Vellore Institute of Technology (VIT)-Vellore, in 2022 (received the Research Excellence Award & Special Achiever’s Award). I conducted my undergraduate thesis under the guidance of <a href="https://uk.linkedin.com/in/ashutoshnatraj" class="uline">Dr. Ashutosh Natraj</a> and <a href="https://research.vit.ac.in/researcher/vasantha-w-b" class="uline">Dr. Vasantha W B</a>. My research focused on <i>"Predictive and Prescriptive analysis for transmission power lines"</i> by leveraging <b>segmentation</b> and <b>object detection</b> models. This work led to the development of <i>VNautilus</i> and <i>VSense</i> applications in Vidrona's application portfolio, receiving the <b><i>ISGF award</i></b> from the <i>Ministry of Power</i>.
  At VIT,  I've had the opportunity to intern at <b>Hewlett Packard & Enterprises</b> (with <a href="https://in.linkedin.com/in/manikandadas" class="uline">Manikanda Das R</a>), <b>Samsung R&D Institute</b> (with <a href="https://www.linkedin.com/in/dr-satya-kumar-vankayala-91161b139/" class="uline">Dr. Satya Kumar Vankayala</a>), and Vidrona (with Dr.Ashutosh Natraj). Additionally, I conducted research at the VIT-Autonomous Research Center (ARC), contributing to lane detection models. In my final semester, I was honored to be a research exchange student (NTU-India Scholar) at Nanyang Technological University, Singapore, where I worked with <a href="https://dr.ntu.edu.sg/cris/rp/rp00965" class="uline">Dr. Xie Ming</a> on <i>Restricted Coulomb Energy</i> (RCE) Neural Networks for Semantic Segmentation in Autonomous Vehicles. 
  <br><br>
  After graduation, I collaborated with <b><i>OpenCV</i></b> for <b><i>Google Summer of Code</i></b> as an open-source developer and contributed to the OpenCV model zoo. My research interests lie in Continual Learning, ExplainableAI, and Multi-Modal Learning to improve generalization and overcome adversarial conditions in AI models. Recently, I have also developed an interest in <b><i>computer graphics</i></b>, and I am eager to expand my expertise into the realm of 3D computer vision.
  <br><br>
  During my free time I like to play the piano, play football (its not soccer), and paint pictures.

  <div class="container">
    <div class="row" style="text-align:center;">
      <div class="col-4">
        <div class="row">
          <div class="col-6">
            <a href="https://vit.ac.in/"><img src="images/VIT.png" style="max-height:150px;width:80%"></a>
          </div>
          <div class="col-6">
            <a href="https://www.hpe.com/in/en/home.html"><img src="images/HPE.png" style="width:175%;max-height:250px"></a>
          </div>
        </div>
      </div>
      <div class="col-8">
        <div class="row" style="text-align:center;">
          <div class="col-3">
            <a href="https://uk.linkedin.com/company/vidrona-ltd"><img src="images/Vidrona.png" style="width:100%;max-height:175px"></a>
          </div>
          <div class="col-3">
            <a href="https://research.samsung.com/sri-b"><img src="images/SamsungResearch.jpeg" style="width:80%;max-height:150px"></a>
          </div>
          <div class="col-3">
            <a href="https://summerofcode.withgoogle.com/archive/2022/organizations/opencv"><img src="images/GSOC.png" style="width:80%;max-height:150px"></a>
          </div>
          <div class="col-3">
            <a href="https://iisc.ac.in/"><img src="images/IISc.jpeg" style="width:80%;max-height:150px"></a>
          </div>
        </div>
      </div>
    </div>
    <div class="row" style="text-align:center;">
      <div class="col-4">
        <div class="row">
          <div class="col-6">
            <div style="padding:10px">
              <span style="font-size: .6rem; font-weight:300;">2018-2022</span>
            </div>
          </div>
          <div class="col-6">
            <div style="padding:10px">
              <span style="font-size: .6rem; font-weight:300;">Mar'21 - Jun'21</span>
            </div>
          </div>
        </div>
      </div>
      <div class="col-8">
        <div class="row" style="text-align:center;">
          <div class="col-3">
            <div style="padding:10px">
              <span style="font-size: .6rem; font-weight:300;">Jul'21 - Dec'21</span>
            </div>
          </div>
          <div class="col-3">
            <div style="padding:10px">
              <span style="font-size: .6rem; font-weight:300;">Jan'22 - Jul'22</span>
            </div>
          </div>
          <div class="col-3">
            <div style="padding:10px">
              <span style="font-size: .6rem; font-weight:300;">Jun'22 - Sep'22</span>
            </div>
          </div>
          <div class="col-3">
            <div style="padding:10px">
              <span style="font-size: .6rem; font-weight:300;">Dec'22 - Dec'23</span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
  
</div>

<div id="research">
<h2><a name="research">Recent Publications</a></h2>
<br/>

<table width="100%" align="center" valign="middle" cellspacing="0" cellpadding="0" style="border-collapse: collapse;">
    <tbody>
      <tr>
      <td width="35%">
        <div class="one" style="text-align:center;">
            <img src="images/poet_teaser.png" style="max-height: 350px;">
        </div>
      </td>
      <td valign="top" width="65%">
        <h5>
        Continual Few-Shot Learning of New Actions With Prompt Tuning
        </h5>
        <p class="authors">
        <font size="-6"><b>Prachi Garg</b>, Joseph K J, Vineeth N B, Necati Cihan Camgoz, Chengde Wan, Kenrick Kin, Weiguang Si, Shugao Ma, Fernando De la Torre</font>
        </p>
        <p>
        Under Review, 2024
        </p>
        <p>
        <a href="https://andrewcmu-my.sharepoint.com/:p:/g/personal/prachiga_andrew_cmu_edu/ERDvr7WAf09Ot48sY0uDF1YB9ksXgJRBpqNmFV9-l1MuYQ?e=7nZDzl">Slides / </a>
        </p>

      </td>
    </tr>
    <tr>
    <td width="35%">
      <div class="one" style="text-align:center;">
          <img src="images/ICCV2023_BOATMI.png" style="max-height: 150px;">
      </div>
    </td>
    <td valign="top" width="60%">
      <h5>
      Data-Free Class-Incremental Hand Gesture Recognition
      </h5>
      <p class="authors">
      <font size="-6">Shubhra Aich*, Jesus Ruiz*, Zhenyu Lu, <b>Prachi Garg</b>, K J Joseph, Alvaro Garcia, Vineeth N B, Kenrick Kin, Chengde Wan, Necati Cihan Camgoz, Shugao Ma, Fernando De La Torre</font>

      </p>
      <p>
      ICCV 2023
      </p>
      <p>
      <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Aich_Data-Free_Class-Incremental_Hand_Gesture_Recognition_ICCV_2023_paper.pdf">Paper / </a>
      <a href="https://github.com/humansensinglab/dfcil-hgr">Code </a>
      </p>

    </td>
  </tr>
      <tr>
      <td width="35%">
        <div class="one" style="text-align:center;">
            <img src="images/maindiagram.png" style="max-height: 350px;">
        </div>
      </td>
      <td valign="top" width="65%">
        <h5>
        Multi-Domain Incremental Learning for Semantic Segmentation
        </h5>
        <p class="authors">
        <font size="-6"><b>Prachi Garg</b>, Rohit Saluja, Vineeth N B, Chetan Arora, Anbumani Subramanian, C V Jawahar</font>
        </p>
        <p>
        WACV 2022
        </p>
        <p>
        <a href="https://openaccess.thecvf.com/content/WACV2022/papers/Garg_Multi-Domain_Incremental_Learning_for_Semantic_Segmentation_WACV_2022_paper.pdf">Paper / </a>
        <i><a href="https://arxiv.org/abs/2110.12205">arxiv / </a></i> 
        <a href="https://www.youtube.com/watch?v=YQC5KLZUpyc">Video / </a>
        <a href="https://github.com/prachigarg23/MDIL-SS">Code / </a>
        <a href="reports/294-wacv-poster.pdf">Poster / </a>
        <a href="https://openaccess.thecvf.com/content/WACV2022/supplemental/Garg_Multi-Domain_Incremental_Learning_WACV_2022_supplemental.pdf">Supplementary</a>
        </p>

      </td>
    </tr>
  </tbody>
</table>
</div>

<br/>

<div id="talks">
<h2>News</h2>
<table width="100%" align="center" valign="middle" cellspacing="0" cellpadding="0" style="border-collapse: collapse;">
    <tbody>
    <td valign="middle" width="20%">
    <tr>
        <h5>
        [Jan 2024] CVPR 2024 Reviewer.
        </h5>
    </tr>
    <tr>
        <h5>
        [Nov 2023] Gave a talk on 'Prompt Tuning for Practical Continual Learning’ at the Multi-Modal Foundation Models course. <a href="https://docs.google.com/presentation/d/17j4nKldmV1EdU9884q-VGBMyN9kDXExH0txUPpM87RY/edit?usp=sharing">[Slides]</a>
        </h5>
    </tr>
    <tr>
        <h5>
        [Oct 2023] Presented our work 'Data-Free Class-Incremental Hand Gesture Recognition' at ICCV 2023 in Paris.
        </h5>
    </tr>
    <tr>
        <h5>
        [Oct 2023] Our work on 'Continual Few-Shot Learning for Activity Recognition' using lightweight prompt tuning is under review!
        </h5>
    </tr>
    <tr>
        <h5>
          [July 2023] Project Leader at CMU CS Pathways, AI Scholars Summer Program. My high school mentees built their first CV-ML project! <a href="https://drive.google.com/file/d/102xu-aLU5xo_cB-5L4DOE-T0AYQfuZyc/view?usp=sharing">[Slides]</a>
        </h5>
    </tr>
    </td>
  </tbody>
</table>
</div>

<br/>

<div id="research projects">
<h2><a name="research">Selected Research Projects</a></h2>
<br/>

<table width="100%" align="center" valign="middle" cellspacing="0" cellpadding="0" style="border-collapse: collapse;">
    <tbody>
    <tr>
    <td width="35%">
      <div class="one" style="text-align:center;">
          <img src="images/diagram-ibm.png" style="max-height: 200px;">
      </div>
    </td>
    <td valign="top" width="65%">
      <h5>
      Towards an AI Infused System for Objectionable Content Detection in OTT [IBM Research Laboratory]
      </h5>
      <p class="authors">
      <font size="-6"> <b>Prachi Garg</b>, Shivang Chopra, Mudit Saxena, Anshu Yadav, Aditya Atri, Nishtha Madaan, Sameep Mehta</font>

      </p>
      <p>
        <a href="https://www.linkedin.com/pulse/towards-ai-infused-system-personalization-video-content-madaan/?articleId=6699399357223218177">Blog-post / </a>
        <a href="https://drive.google.com/drive/folders/1DVUCCE7OLlA7nFbgsf3VanIH_pZRprrq?usp=sharing">Demo </a>
        <a href="https://kidify-ibm.herokuapp.com/survey.html">Survey</a> 
      </p>
      <p>With the substantial increase in the consumption of OTT content in recent years, personalized objectionable content detection and filtering has become pertinent for making movie and TV series content suitable for family or children viewing. We propose an objectionable content detection framework which leverages multiple modalities like (i) videos, (ii) subtitle text and (iii) audio to detect (a) violence, (b) explicit NSFW content, and (c) offensive speech in videos. </p>
      <a href="https://www.linkedin.com/pulse/towards-ai-infused-system-personalization-video-content-madaan/?articleId=6699399357223218177" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Blog-post</a>
      <a href="https://drive.google.com/drive/folders/1DVUCCE7OLlA7nFbgsf3VanIH_pZRprrq?usp=sharing" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Demo</a>
      <a href="https://kidify-ibm.herokuapp.com/survey.html" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Survey</a> 
    </td>
  </tr>
  <tr>
  <td width="35%">
    <div class="one" style="text-align:center;">
        <img src="images/greyc-proposed-model.png" style="max-height: 200px;">
    </div>
  </td>
  <td valign="top" width="65%">
    <h5>
    Memorization and Generalization in CNNs using Soft Gating Mechanisms [Image Team GREYC, University of Caen Normandy]
    </h5>
    <p class="authors">
    <font size="-6"><b>Prachi Garg</b>, Shivang Agarwal, Alexis Lechervy, Frederic Jurie</font>
    </p>
    <p>
    <a href="reports/Report-GREYC.pdf">Technical Report / </a>
    <a href="https://github.com/prachigarg23/Memorisation-and-Generalisation-in-Deep-CNNs-Using-Soft-Gating-Mechanisms">Code / </a>
    <a href="reports/report-failed-models-GREYC.pdf">Technical Report, Suboptimal ResNet Gating Mechanisms</a>
    </p>
    <p>A deep neural network learns patterns to hypothesize a large subset of samples that lie in-distribution and it memorises any out-of-distribution samples. While fitting to noise, the generalisation error increases and the DNN performs poorly on test set. In this work, we aim to examine if dedicating different layers to the generalizable and memorizable samples in a DNN could simplify the decision boundary learnt by the network and lead to improved generalization in DNNs. While the initial layers that are common to all examples tend to learn general patterns, we dedicate certain deeper additional layers in the network to memorise the out-of-distribution examples.</p>
    <a href="reports/Report-GREYC.pdf" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Report</a>
    <a href="https://github.com/prachigarg23/Memorisation-and-Generalisation-in-Deep-CNNs-Using-Soft-Gating-Mechanisms" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Code</a>
    <a href="reports/report-failed-models-GREYC.pdf" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Suboptimal ResNet Gating Mechanisms</a> 
  </td>
  </tr>
  </tbody>
</table>
</div>

ML-GAT: Multi-label Node Classification using Enhanced Graph Attention Network
Prachi Garg*, Ashi Gupta*, Rajni Jindal

Many real-world graph based problems require the assignment of more than one label to each node instance in the graph. We study here multi-label node classification using enhanced graph neural networks. We propose a novel architecture, Multi-Label Graph attention Network (ML-GAT) that leverages the applicability of the attention based Graph Attention Network (GAT) to efficient inductive semi-supervised multi-label classification by augmenting complex inter-label and node-label dependencies implicit in the graph structure to the learning process. Our model achieves 15.01% increase over the current state-of-the-art ML-GCN framework for Facebook dataset and 6% increase for the Yeast dataset. We analyse the influence of dropout and training size; and infer the relative importance of node-label and label-label dependencies.

*Denotes equal contribution 

Visual Wildlife Monitoring: Domain Generalization for Animal Detection in the Wild
Prachi Garg, Gullal Cheema, Saket Anand

I worked towards benchmarking species detection in camera trap images from unconstrained wild environments to generalise to new environments using state-of-the-art Faster-RCNN variants. The Catech Camera Traps dataset (CCT20) is an unconstrained wild environment camera traps dataset designed to study domain generalization for animal species. It contains test data collected from both, locations that are same as train data (cis) as well as locations different from train data (trans). Factors like illumination, motion blur, occlusion, camouflage and perspective can severely affect the performance of species recognition systems. We bridged the generalization gap between cis-locations (test domain same as train) and trans-locations (unseen test domain) performance from 26.8% to 22.9% by using state-of-the-art Faster-RCNN variants.



<p> I have also worked on Domain Generalization in Animal Detection for visual wiildlife monitoring, and multi-label node classification in Graph Attention Networks.</p> 
<hr>
<p align="right">
<small>Website template from <a href="https://github.com/johno/pixyll">here</a> and inspired from <a href="https://virajprabhu.github.io/">here</a> </small></p>

